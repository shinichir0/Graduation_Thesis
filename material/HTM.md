<!-- toc orderedList:0 depthFrom:1 depthTo:6 -->

* [非教師ニューラルネットワークモデルを用いた連続オンライン系列学習](#非教師ニューラルネットワークモデルを用いた連続オンライン系列学習)
  * [概要](#概要)
  * [1. 導入](#1-導入)
  * [2. 実時間ストリーミングデータの分析への挑戦](#2-実時間ストリーミングデータの分析への挑戦)
    * [2-1. 連続学習](#2-1-連続学習)
    * [2-2. 高次予測](#2-2-高次予測)
    * [2-3. 並列同時予測](#2-3-並列同時予測)
    * [2-4. ノイズに対する頑強性・過失許容性](#2-4-ノイズに対する頑強性過失許容性)
    * [2-5. ハイパーパラメータチューニングなし](#2-5-ハイパーパラメータチューニングなし)
  * [3. HTM系列記憶](#3-htm系列記憶)
    * [3-1. HTMニューロンモデル](#3-1-htmニューロンモデル)
    * [3-2. 2つの分離したSparse Representations](#3-2-2つの分離したsparse-representations)
    * [3-3. HTM活性化と学習規則](#3-3-htm活性化と学習規則)
    * [3-4. SDR(Sparse Distributed Representations) Encoder and Classfier](#3-4-sdrsparse-distributed-representations-encoder-and-classfier)
  * [4. 人工データを用いた高次系列予測](#4-人工データを用いた高次系列予測)
    * [4-1. ストリーミングデータの連続的オンライン学習](#4-1-ストリーミングデータの連続的オンライン学習)
    * [4-2. データストリームの変更への適応](#4-2-データストリームの変更への適応)
    * [4-3. 同時並列予測](#4-3-同時並列予測)
    * [4-4. 高次系列の長期間依存の学習](#4-4-高次系列の長期間依存の学習)
    * [4-5. 時間的ノイズによる高次の分裂](#4-5-時間的ノイズによる高次の分裂)
    * [4-6. ネットワークの損失に対する頑強性](#4-6-ネットワークの損失に対する頑強性)
  * [5. ニューヨーク市のタクシーにおける乗客の要求の予測](#5-ニューヨーク市のタクシーにおける乗客の要求の予測)
  * [6. 考察・結論](#6-考察結論)
    * [6-1. ストリーミングデータの連続的な学習](#6-1-ストリーミングデータの連続的な学習)
    * [6-2. SDRを用いた系列学習](#6-2-sdrを用いた系列学習)
    * [6-3. 頑強性・一般化](#6-3-頑強性一般化)

<!-- tocstop -->

# 非教師ニューラルネットワークモデルを用いた連続オンライン系列学習
## 概要
感覚神経による入力の時間的な系列を認識し予測する能力は自然環境で生き抜くのに非常に重要なものである。
大脳新皮質の神経細胞における多くの知見をもとにして**Hierarchical Temporal Memory (HTM)** (階層時間的記憶)が大脳新皮質における系列学習の理論的枠組みとして提案された。
この論文においてはHTMの系列記憶の特性を分析しストリーミングデータの時系列予測、学習に適用した。
私たちはこのモデルは非教師のHebbianのような学習規則を用いて連続的に膨大な可変順序な時間的な系列の学習が可能であることを示す。
またこのモデルによって成形された時間的なSparse Coding(NeuralNetにおける重要な重みの探索)は並列予測によって十分に曖昧さがなくなったと言えるまで調整することによって枝分かれした時間的系列を頑強に扱える。
私たちはHTMの時間的記憶を他の系列学習アルゴリズムと時系列予測タスクを人工のデータと実世界データを用いて比較した。
1. 統計的手法(自己回帰移動平均)
1. ニューラルネットワーク(時間遅延NN、オンライン系列機械学習)
1. Recurrent Neural Network(Long Short-Term Memory、Echo-State Networks)

このHTMのモデルは他の最先端のアルゴリズムらよりも高い精度を打ち立てた。さらにHTMのモデルは連続的なオンライン学習を含む系列学習において重要な特質を示した。それは高次的な統計やノイズに対する頑強性、過失許容性、タスクに特化したハイパーパラメータチューニングなしでの高性能といったものとともに枝分かれした系列に対して並列的な予測が可能であるといったことである。
さらに言えばHTMの系列記憶は、脳がどのように時系列タスクを解決するのかということについての私たちの理解を先んじているというだけでなく、実世界の連続したデータストリーミングを用いた時系列学習タスクにも適用できる。
## 1. 導入
自然環境において大脳新皮質は連続的に感覚情報の流れを処理し、大きな時間的空間的な世界モデルを構築している。
順序立てられた時間的な系列を認識し予測する能力は会話認識、感覚近くの有効性、自然な視覚といった脳のほとんど全ての機能に欠かすことができない。
神経画像の研究によって多数の大脳新皮質の領域が時間的な系列の処理に関わっていることがわかっている。
また最近の神経生理学の知見によると第一視覚野の全てのニューロンが時間的空間的な系列の認識と予測を学習していることがわかっている。
第一視覚野と聴覚野のニューロンは系列感受性も示している。
これらの研究は系列学習が多くの大脳新皮質の領域によって解決される重要な問題であることを示している。

機械学習の研究者は神経科学とは別に系列学習における素晴らしい成果を出している。
隠れマルコフモデルや自己回帰移動平均といった統計的手法がそれぞれ時間的パターン認識や時系列予測に用いられている。
数多くのニューラルネットワークが系列データに対するモデルとして提案されている。
時間遅延ニューラルネットワーク(TDNN)が入力の遅延を繰り返すことで系列データを用いるモデルとなっていた。
再帰的ニューラルネットワーク(RNN)は系列機構に対して側面の再帰的な接続によって１時間に１つの記憶を処理することができる。
例えば長期短期記憶(LSTM)は情報へのアクセス時間を選択的に通すことにができ、ゲート機構によって長期的な依存関係をモデルできたために、現実世界の幅広い問題に対して高い精度を持っている。
Echo-State Network(ESN)は再帰的なネットワークをランダムに接続することで動的な貯水池として用い、系列を反応信号による訓練可能な線形和でモデルしている。

機械学習アルゴリズムらは大脳新皮質のアルゴリズムの知見をすこしでも
利用できているであろうか。
現在の最先端の統計分析や機械学習アルゴリズムはベンチマークで計測する問題においては素晴らしい予測を成し遂げているが、ほとんどの時間系列の予測タスクは動的で不定常なシナリオに対する性能に注目していない。
ベンチマークは形式的に訓練とテストのデータセットを分けている。それは潜在的にテストデータと訓練データに共有する類似性があると仮定しているからである。
それに対して脳における系列学習では連続的にノイズが含まれていたり常に感覚入力信号が変わっていったりするものを扱う必要がある。
とりわけ、入手可能なストリーミングデータが増えていく時にオンラインの系列学習アルゴリズムが複雑でノイズを含むデータストリーミングに対応できるように要求も増えていく。
さらに言えば、脳が用いている計算原理の解析は多くの機械学習アプリケーションで重要になっている系列学習問題においてさらなる知見を与えてくれるといえる。

脳の系列メモリの根底にある正確なニューロンのメカニズムはいまだ不明であるが、スパイキングニューロン（ニューロンの発火頻度ではなく内部電位に注目したもの）生物学のもっともらしいモデルが研究されてはいる。
たとえば、ラオらはSpike-timing-dependent plasticity(STDP)(脳のニューロン間の接続の強さを調節する生物学的処理)が大脳新皮質の再帰的な回路において予測系列タスクを可能にすると示した。
Spiking recurrent network models は教師あり学習を用いた時間的な系列を正確に認識し、呼び戻すことができることも示された。
これらの研究は生物学的なモデルで解けるような形式に制限して演習されていた。
しかしながら、これらのモデルは比較的シンプルで限定された形式の系列のみを認識するためにほとんどの実践的な系列学習のアプリケーションはSpiking network modelsを使用しない。
これらのモデルたちは現実世界の問題に対する非生物学的な統計や機械学習のアプローチの性能にもおよばない。

この論文では、私達は大脳新皮質の系列学習の詳細なモデルであるHTM系列記憶を比較研究する。
HTMのニューロンモデルは多くの最近発見された特性やピラミッド型の細胞群や樹状突起の活性化といったものを複合させている。
複雑な系列はsparse distributed temporal codes(疎分散時間符号化) によって表され、このネットワークはHebbianのような非教師学習規則によって訓練される。
このアルゴリズムは分離した連続的な系列の予測や例外の探知、系列の認識と分類といった多くの現実問題に適用している。

私達はHTM系列記憶を４つの有力な統計的手法や機械学習技術と比較した。 1つ目はARIMA(自己回帰和分移動平均)という時系列予測の統計的手法。２つ目は順伝播ニューラルネットワークを用いた系列オンライン学習といった先端機械学習。
３つ目は２つのLSTM,ESNといった再帰的ニューラルネットワークである。
私達はHTM系列記憶がこれらの他の手法に対してより正確な予測を達成したことを示す。
それに加えて、これが現実世界におけるストリーミングデータからの系列学習にとって望ましい様々な特徴を示している。
私達はHTMネットワークがデータストリーミングから複雑な高次の系列から学習し、素早くデータの統計に適用し、自然に並列予測を枝分かれした系列にたいしておこない、システムの過失に対して許容性を持っていることをデモした。

この論文は以下のように続く。２章では現実のストリーミングデータの分析にとって望ましいとされる系列学習アルゴリズムのさまざまな特性について考察する。３章ではHTM時間記憶モデルについて紹介する。４章と５章ではHTM時間記憶をそれぞれ分離した人工データと連続した現実世界のデータといった他の系列学習アルゴリズムに適用する。６章で考察と結論について述べる。
## 2. 実時間ストリーミングデータの分析への挑戦
ストリーミングデータが入手しやすくなってくるにつれて、オンラインの系列学習への需要が高まっている。
ここで、データストリームは情報記憶の順序立てられた系列であり計算や記憶の容量に制限がある中で処理されるべきものとする。
データストリームの分野における目的はネットワークのトラフィックやセンサーデータや金融取引といったしばしばその統計を変化させるような連続したデータストリームから知識を引き出すことである。
現実世界におけるそのようなノイズが多く複雑な系列学習において必要なのは正確な予測に加えて他にも多くの特性である。
これは静的なデータセットに対して最適な精度をもつように発展し、現実世界のストリーミングデータの分析タスクを解析するための柔軟性を欠いている多くの機械学習とは対象的である。

これらのアルゴリズムらと対象的に大脳新皮質では系列学習問題を徹底的に異なった方法で解決している。
特定の問題に対して最適な精度を達成するというよりも、大脳新皮質はノイズを含む感覚入力のストリームから連続的に学習し、素早くデータの統計の変化に適応する。
情報が不十分だったり不明瞭だったりした時でも、大脳新皮質は利用可能な感覚情報から並行してもっとらしい予測を立てられる。

データストリームからの実時間系列学習は機械学習アルゴリズムに対する他に類を見ない挑戦を提供する。
予測の正確性に加えて生命システムと実時間ストリーミングアプリケーションに適用できるような数々の基準をあげる。
### 2-1. 連続学習
連続的なデータストリームはしばしば統計を変化させる。
結果的にアルゴリズムはデータストリームから連続的に学習しつつ素早く変化に適用する必要がある。
この特性は連続的な感覚情報の流れを処理するのに重要だが、機械学習ではあまり学習されなかった。
実時間のデータストリームの解析においてはアルゴリズムが新しいパターンを素早く認識し学習することができるかどうかが重要である。
機械学習アルゴリズムはバッチ学習かオンライン学習に分類される。
どっちのアルゴリズムでも連続的な学習アプリケーションに適用できる。
バッチ学習を連続的なデータストリームの分析に適用するには過去のデータの記録をバッファしたデータセットを持ち続ける必要がある。
このモデルでは一定の間隔で再学習を行うが、それはデータの統計が時間を置くと変化しうるからだ。
バッチ学習の例では潜在的に膨大な計算量と記憶空間を必要とする。特にデータ速度が高い場合において。
しかしオンライン系列アルゴリズムは１つずつ学習するためバッファデータセットを必要としない。
### 2-2. 高次予測
実世界における系列は複数のタイムステップにわたる前後関係依存を含んでいる(例えば高次予測をする能力)。
その間隔はマルコフモデルに従い、特に正確な予測のために過去のタイムステップアルゴリズムの最小の数を考慮する必要がある。
理想的なアルゴリズムは次元を自動で効果的に学習すべきである。
### 2-3. 並列同時予測
時間的な前後関係が与えられた時に、将来の出力予測は複数のものがありうる。
実世界のデータでは情報が不明瞭なため一つのベストな予測のみを考慮するには不十分なことがありうる。
より良い系列学習アルゴリズムというのは並列同時予測とオンラインでそれぞれの予測の見込みを評価するというものである。
このためにはアルゴリズムはありうる将来における出力の分布を出力する必要がある。
この特性は隠れマルコフモデルや再帰的ニューラルネットワークで実現していたが、最尤予測に限定される自己回帰移動平均のような他のアプローチでは実現できない。
### 2-4. ノイズに対する頑強性・過失許容性
実世界における系列学習はセンサーノイズやデータ伝送エラーや固有のデバイス制限によるデータの不正確さや欠損が頻繁に発生するノイズの多いデータソースを扱う。
良い系列学習アルゴリズムは入力のノイズに対する頑強性を持つべきである。

系列学習アルゴリズムはニューラルネットワークにおけるシナプスやニューロンの欠損といったようなシステム障害が発生した場合にも適切に学習する必要がある。
欠損許容生や失敗に対する頑強性といった特性は脳には存在するものであり、次世代のニューロモーフィック(神経形態学的)な処理装置の発展において重要である。
ノイズに対する頑強性と過失許容性は幅広い様々な問題に対する柔軟性とアルゴリズムの適応性を保証する。
### 2-5. ハイパーパラメータチューニングなし
大脳新皮質の学習は幅広い問題に対する極めて頑強である。
それに対してほとんどの機械学習アルゴリズムはそれぞれのタスクに対するハイパーパラメータの組み合わせによる最適化の必要がある。
それは通常、交差検証データセットにおける性能測定基準に基づいて、手動で指定されたハイパーパラメータ空間のサブセットを検索することを伴う。
ハイパーパラメータチューニングはデータストリームマイニングのような高度な自動化を必要とするアプリケーションに対するもっとも大きな関門となっている。
理想的なアルゴリズムはどんなタスク特化のハイパーパラメータチューニングなしで幅広い問題に対する満足な精度を持つべきである。

---
既存の機械学習技術の多くはこれらの特性を様々な度合いで示している。
ストリーミング解析に対して本当に柔軟で強力なシステムとはこれら全てに対して対応するはずである。
この論文の残りではHTM系列メモリを上で述べた基準を用いた様々なタスクにおいて他の系列学習アルゴリズムと比較する。(ARIMA, ELM, ESN, TDNN, and LSTM)
## 3. HTM系列記憶
この章ではHTM系列メモリの計算的な詳細を明らかにする。
私たちはまずニューロンモデルについて述べる。
そして私達は高次系列の表現を記述し、その後学習規則の正式な記述も行う。
またその記述の中でいくつかの関連する神経生理学の実験による知見についても指摘する。
生物学への詳細なマッピングはHawkinsとAhmadにある。
### 3-1. HTMニューロンモデル
HTMニューロンは最近の神経生理学の発見である大脳皮質のニューロンと樹状突起の機能に関するものに影響され、非線形のシナプス統合を実装している。
ネットワーク内のそれぞれのニューロンが２つの分離した領域をふくんでいる。
ひとつの樹枝状の区分を持つ中心部分とまとまりになっている独立した樹枝状の区分をもつ末梢部分である。
それぞれの区分がシナプスの集合をもっている。
シナプスのソースは領域によって異なっている。
中心部分のシナプスはその層の中に入るフィードフォワード入力を表しているのに対して、末梢部分は上位領域からのフィードバック接続と層内からの側面の接続を表している。
この論文内では１層について考えるためフィードバック接続は無視する。

それぞれの末梢部分は層内の他のニューロンとの側面のシナプス接続のあつまりを含んでいる。
あるセグメントで同時に活性化している接続の数がスレッショルドを超えるとセグメントが活性化する。
セグメントが活性化するのは細胞が発火することが原因ではなく、代わりに私達が予測状態と呼ぶ細胞の脱分極状態になったことが原因となる。
このようにして、それぞれのニューロンは特定の時間的な前後関係を見つけ出し、その前後関係に基づいた予測を行う。
それぞれのニューロンは内部状態として活性化状態、予測状態、非活性化状態の３つのうちの１つをとる。
出力は常に活性化か非活性化の２つである。

このニューロンモデルは数々の最近の実験による発見がほとんどのニューラルネットワークモデルのように単純に入力の重み付き総和によって発火するかやどう働くかを決めているのではないのではないかということを示していることに影響されている。
そのかわりに樹状枝は活性化処理の要素になっている。
樹状枝上の空間的時間的に近接したいくつかのシナプスの活性化は局所的なNMDAスパイクを開始させることができ、次いで細胞体の有意で持続的な脱分極を引き起こす。

---
図１:HTM系列メモリモデル
(A) 大脳新皮質は６つの層からなり、それぞれの層はミニカラムの集合をもち、それぞれのミニカラムは多層の細胞を含む。
(B) HTMニューロンモデルは３つの別れた樹状状の統合領域を持つ。
HTMニューロンは樹状突起とNMDAスパイクを一連のシナプスを伴う同時検出器の配列としてモデル化している。
末梢樹状突起上の１組のシナプスの共活性化はNMDAスパイクをひきおこし、細胞体の脱分極もおこす。（予測状態）
(C, D) 共有サブシーケンスによる高次マルコフシーケンスの学習(A,B,C,D vs X,B,C,Y)。
それぞれの系列要素はカラム内接続の禁止により、疎なミニカラムの組を呼び出す。
(C) 系列学習に先立ち、ミニカラム内のすべてのセルが活性化する。
(D) 学習後、側方からの接続によって脱分極した細胞はより速く活性化し、同じカラム内の他の細胞が細胞内阻害を介して発火するのを防止する。
このモデルは２つの同時状態をもつ。
１つは現在のフィードフォワード入力を表すミニカラムレベルにあり、もう１つは入力の前後関係を表す個々の細胞レベルにある。
異なる細胞は２つの配列（C',C")においてCに応答するので、DまたはYの正しい高次予測を呼び出すことができる。

---
### 3-2. 2つの分離したSparse Representations
HTMネットワークはカラムの集合からなるHTMニューロンの層を含んでいる。
このネットワークは2つの分離したSparse Representationsの構成を用いて高次系列を表している。
いかなる時も現在のフィードフォワード入力と以前の系列の前後関係の両方がSparse Representationsを用いて表現される。

１つ目の表現は列レベルである。
列内の全てのニューロンは中心部の樹状突起において同じフィードフォワード入力のパターンを検出すると仮定する。
同じ層内のカラム結合の禁止するメカニズムを介して、各入力要素は任意の時点におけるSparse Representationsによるカラムの活性化として符号化される。
いかなる時ももっとも活性化しているフィードフォワード入力を受け取った上位２％のカラムが活性化する。

２つ目の表現はこれらのカラム内の個々の細胞レベルである。
任意の与えられた時点で、活性化しているカラム内の細胞のサブセットは、現在のパターンの学習された時間的前後関係に関する情報を表す。
これらの細胞は同じネットワーク内の他の細胞への側方からの投影を通して、今後の入力の予測につながる。
細胞内の予測状態はカラム内の接続禁止をコントロールする。
ある列が予測された細胞を含み、後に十分なフィードフォワード入力を受け取った場合、これらの予測された細胞は活性化し、そのカラム内のほかの細胞の活性化を禁止する。
予測状態になっている細胞が１つもないとき、カラム内のすべての細胞は活性化する。

これらの表現の背後にある直観を説明するために、２つの抽象的な系列であるA-B-C-DとX-B-C-Yを考慮する。
この例ではCに続く正確な予測を行うためにはAまたはXで始まる系列を記憶しておくことが必要である。
現在の入力は活性化している細胞（黒い点）を含むカラムのサブセットによって表されている。
この活性化しているカラムのセットは時間的な前後関係に依存しないが、現在の入力のちょうど上にある。
学習後、このカラムのサブセット内の異なる細胞は過去の前後関係に基づく予測に応じて活性化する。
これらの細胞はCを表すカラムへの側方からの接続を含む細胞の集合に基づいてCに続く要素の予測（DまたはY）を導く。

この二重表現パラダイムは、多くの興味深い特性をもたらす。
１つ目は、疎な表現の使用によってモデルが複数の予測を同時に行うことを可能にするということである。
例えば、入力Bを前後関係なしでネットワークに入力すると、Bの入力を表す列のすべての細胞が発火し、C'とC"の両方の予測につながる。
２つ目は情報が分散という方法によって複数の細胞の共活性化によって保存されるために、このモデルは入力内のノイズやニューロンやシナプスの損失といったようなシステムの障害にたいしても自然に頑強であるということである。
### 3-3. HTM活性化と学習規則
これまでの章ではネットワークの動作を直感的に説明した。
この章ではHTMネットワークの正式な活性化と学習の規則について説明する。
ネットワークがN個のカラムを持ち、１つのカラム内にM個のニューロンを持つをして考える。
時間ステップｔにおける活性化状態をM＊Nの２進数行列$A^t$で表し、ここで $a^t_{ij}$はｊカラムのi番目の細胞の活性化状態を表している。
同様に、時間ステップtにおける予測状態をM＊Nの２進数行列$\Pi^t$で表し、
ここで$\pi^t_{ij}$はｊカラムのi番目の細胞の予測状態を表している。
それぞれのシナプスをスカラー不変値でモデル化し、その不変値が接続のスレッショルドを超えた場合にシナプスがつながっていると考える。
M*Nの２進数行列$D^d_{ij}$でj番目のカラムのi番目の細胞のd番目のセグメントの不変値を表す。
シナプスの不変値行列は０と１の間に境界が定められている。
２進数でシナプスが接続しているかだけをあらわした$\tilde{D^d_{ij}}$を用いる。
このネットワークは各セグメントが層内の細胞のランダムに選択されたサブセットに対する潜在的なシナプスのセット（すなわち、非ゼロの不変値を有する）を含むように初期化できる。
シミュレーションを高速化するために、すべてのセグメントとすべてのセルのシナプスのセットを明示的に初期化するのではなく、実行時に貪欲にセグメントを作成する。

ニューロンの予測状態は以下のように扱われる。
もし、樹状突起のセグメントが十分な入力を受け取ったら、それが活性化し、続いて即時のスパイクを引き起こすことなく細胞体を脱分極させる。
数学的に表すと、時間ステップtにおける予測状態は以下のようになる。
スレッショルドはセグメントの活性化スレッショルドであり、丸は要素ごとの席である。
末梢シナプスは同じ層内の以前活性化した細胞からの入力を受け取るため、過去の入力の文脈情報を含み、将来の入力を正確に予測するために使用することができる。

いずれの時点においても同一カラム間の接続禁止プロセスは現在のフィードフォワード入力パターンに最もよく一致する疎なカラムのセットを選択する。
私達は各カラムにおける活性化している中心シナプスの数を計算し、最も多くのシナプス入力を受け取るカラムの上位２％を活性化する。
このセットを$W^t$で表す。
中心シナプスは各カラムが入力の５０％にランダムで接続されるように初期化された。
この論文では系列学習に焦点を当てているため、中心シナプスの値は学習中は固定されていました。
原則的に中心シナプスは空間的な競合学習規則にしたがって、学習中に連続的に適応することもできる。

予測状態のニューロン（すなわち脱分極）は、同じフィードフォワード入力を受信する他のニューロンよりも競争上有利である。
具体的には脱分極した細胞は、引き続き十分なフィードフォワード入力を受け取る場合に他の分極していない細胞よりも速く発火する。
速く発火することにより、同じカラム内の隣接する細胞がカラム内阻害によって活性化するのを防ぎます。
各細胞の活性化状態は次のように計算される。
式３．２の１つ目の条件式は前の時間ステップ中に予測状態にあった場合に当選カラム内の有効になる細胞を表している。
当選カラム内のすべての細胞が予測状態にない場合、式３．２の２つ目の条件式のようにそのカラムのすべてのセルがアクティブになります。

系列記憶モデルにおける横方向の接続は、Hebbianのような規則を用いて学習される。
具体的には、細胞が脱分極し、その後活性化すると、脱分極を引き起こした樹状細胞セグメントを強化する。
活性化しているカラムの細胞が予測されない場合は、最も活性化されたセグメントを持つセルを選択し、そのセグメントを強化する。
樹状突起セグメントの強化は、不活性シナプスの不変値を小さな値p-に減少させ、活性シナプスの不変値をより大きな値p+に増加させることを含む。
$\dot{D^d_{ij}}$は${D^d_{ij}}$の正の値のみを取り出したものである。

私達はまた、活性化しなかった細胞の活性化したセグメントに小さな欠落をあたえることで長期のうつ病の効果を模倣する。

学習規則は神経生理学の研究である活動依存性シナプス形成からインスピレーションを受けており、大人の大脳新皮質は知覚活動に迅速に応答して新たなシナプスを生成することを示している。
私たちが選んだ数学式は、このHebbianシナプス形成学習のルールになぞらえられる。
コスト関数の勾配降下を実装してルールを導出しなかった。
同様のまたはより良い結果をもたらす他の数学的処方が存在し得る。

パラメータの完全なセットと実装の詳細については、付録を参照のこと。
これらのパラメータは、疎な分散表現の特性に基づいて設定されました。
特筆すべきことにこの論文中の様々なタイプの系列学習タスクの全てに対して同じセットのパラメータを使用した。
### 3-4. SDR(Sparse Distributed Representations) Encoder and Classfier
HTM系列記憶は内部的に疎な分散表現で動作する。
実世界の系列学習の問題にHTMを適用するためには、まずエンコーダを使用して元のデータをSDRに変換する必要があります（図2参照）。
さまざまな種類のデータを処理するための異なるエンコーダを作成しました（Purdy、2016）。
本論文内では、タクシー乗客予測実験のために分類別データとスカラー量と日時用のエンコーダにランダムなSDRエンコーダを使用しました。

HTMの出力SDRから予測値をデコードするために、我々は2つの分類器(SDRと重複する単純な識別器・最尤識別器)を検討した。
単一ステップ離散シーケンス予測タスクでは、観測されたすべての要素のSDRと予測された細胞の重なりを計算し、重複が最も高いものを選択しました。
連続スカラー値予測タスクでは、スカラー値の全範囲を22個の独立したバケットに分割し、単一層のフィードフォワード分類ネットワークを使用しました。
細胞の活性化パターンxの大きな配列が与えられると、分類ネットワークは、softmax活性化関数を使用して、すべての可能なクラスにわたって確率分布を計算する（Bridle、1989）。
可能なクラスの数だけ出力単位があります。
j番目の出力ユニットは、すべての入力の重み付き総和を受け取り、$w_{ij}$はi番目の入力ニューロンからj番目の出力ニューロンへの接続重みです。
推定されるクラス確率は、出力ユニットの活性化レベルによって与えられる。

最尤最適化を用いて、重み行列wの学習規則を導出した。
$z_j$は観測された（目標）分布であり、$\lambda$は学習率である。
xは非常に疎なので、いつでも重み行列のごくわずかな部分を更新するだけで良いことを述べたい。
したがって、重み行列の次元が高いにもかかわらず、分類器の学習アルゴリズムは高速である。
## 4. 人工データを用いた高次系列予測
### 4-1. ストリーミングデータの連続的オンライン学習
### 4-2. データストリームの変更への適応
### 4-3. 同時並列予測
### 4-4. 高次系列の長期間依存の学習
### 4-5. 時間的ノイズによる高次の分裂
### 4-6. ネットワークの損失に対する頑強性
## 5. ニューヨーク市のタクシーにおける乗客の要求の予測
## 6. 考察・結論
### 6-1. ストリーミングデータの連続的な学習
### 6-2. SDRを用いた系列学習
### 6-3. 頑強性・一般化
